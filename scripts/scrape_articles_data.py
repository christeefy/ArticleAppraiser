import numpy as np
import pandas as pd
import scholarly
import time
import random

# Add path reference to `src`
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parents[1]))

from src.utils import tqdm_f


def clean_text(text: str) -> str:
    '''
    Clean text by stripping whitespaces and lowercasing it.

    Arguments:
        text {str} -- Text

    Returns:
        str -- Cleaned text.
    '''
    return text.strip().lower()



def prepare_articles_df(src: str = 'data/dataframes/main_df.pkl',
                        dst: str = 'data/scraped/articles.pkl'
                        ) -> None:
    '''
    Create a dataframe containing the title and citedby for each article.

    Keyword Arguments:
        src {str}
        -- Path to training dataframe (default:{'data/dataframes/main_df.pkl'})
        dst {str}
        -- Path to save articles. (default: {'data/scraped/articles.pkl'})

    Raises:
        TypeError -- If args do not point to pickle files.
        FileExistsError -- If save path already exists.

    Returns:
        pd.DataFrame
    '''
    if Path(src).suffix != '.pkl':
        raise TypeError('`src must be a pickle file.')
    if Path(dst).suffix != '.pkl':
        raise TypeError('`dst` must be a pickle file of extension .pkl')
    if Path(dst).is_dir():
        raise FileExistsError(f'File exists at {src}.')

    # Load and preprocess data
    df = pd.read_pickle(src)
    df['citedby'] = np.nan

    articles = df[['title', 'citedby']]

    # Save file
    articles.to_pickle(dst)


def scrape(src: str = 'data/scraped/articles.pkl'):
    '''
    Scrape articles 'citedby' data.

    Keyword Arguments:
        src {str} -- Path to articles dataframe
                     (default: {'data/scraped/articles.pkl'})

    Raises:
        TypeError -- If file does not exists at `src`.
        ConnectionError -- If a Captcha is generated by G. Scholar.
    '''

    # Get scholar data
    if not Path(src).is_file():
        print('Articles file is not found. '
              f'A new one will be generated at {src}.')
        prepare_articles_df()
    if Path(src).suffix != '.pkl':
        raise TypeError('`src` must be a pickle file of extension .pkl')

    df = pd.read_pickle(src)

    # Modify column type to enable insertion of non-float values
    df['citedby'] = df['citedby'].astype(object)

    rows_to_scrape = df['citedby'].isna()

    # Check each article in the dataframe
    for i in tqdm_f(is_range=False)(df.index,
                                    desc='scraping articles',
                                    total=len(df)):
        if not rows_to_scrape[i]:
            continue

        try:
            # Find article on Google Scholar
            article_name = df.at[i, 'title']
            query = scholarly.search_pubs_query(article_name)

            if type(query).__name__ != 'Publication':
                raise ConnectionError('Captcha activated.')

            # Find the exact article
            if clean_text(query.bib['title']) == clean_text(article_name):
                df.at[i, 'citedby'] = (
                    query.citedby if hasattr(query, 'citedby') else 'N/A'
                )
            else:
                df.at[i, 'citedby'] = 'N/A'

        except KeyboardInterrupt as e:
            # Handle interrupts gracefully
            print('\nKeyboard interrupt detected! Saving dataframe...')
            df.to_pickle(src)
            sys.exit(0)
        except ConnectionError as e:
            # Pause program
            print(e)
            df.to_pickle(src)
            time.sleep(1800 + random.uniform(0, 300))


if __name__ == '__main__':
    scrape()
